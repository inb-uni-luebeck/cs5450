{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please insert the names of all participating students:\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "  if os.getcwd() == '/content':\n",
    "    !git clone 'https://github.com/inb-luebeck/cs5450.git'\n",
    "    os.chdir('cs5450')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1: \n",
    "\n",
    "In this exercise, we will use the adaptive boosting algorithm [AdaBoost](https://en.wikipedia.org/wiki/AdaBoost) to combine multiple weak classification models to one strong classifier for a two-class classification problem\n",
    "on a two-dimensional dataset. Our simple classifier will be a threshold on either dimension, dividing the dataset into two classes.\n",
    "The data will be generated with the provided function genData. Each row in the generated matrix data corresponds to one observation and each column to a single feature. The generated vector labels contains the corresponding desired output value of the two-class classification problem. \n",
    "\n",
    "When done correctly, your code should do the following:\n",
    "1. Create a random dataset.\n",
    "2. Iterativly, do the following:\n",
    "    <br>a) Train another weak classification model.\n",
    "    <br>b) Compute the predictions of the weak classifier.\n",
    "    <br>c) Calculate the weighted training error of the weak classifier.\n",
    "    <br>d) Update the distribution of weights assigned to each data sample.\n",
    "    <br>e) Combine all the weak classification models to one strong classifier.\n",
    "    <br>f) Calculate the error of the strong classifier.\n",
    "    <br>g) Compute the decision boundary of the strong classifier.\n",
    "3. Plot the error of the strong classifier for each iteration.\n",
    "4. Create an animation which shows how the decision boundary of the strong classifiers changes over time.\n",
    "\n",
    "In case you are struggeling with the task, here are some helpful tips and hints:\n",
    "1. Useful functions : gen_data, train_classifier, get_decision_boundary\n",
    "2. Good initial values are: n_samples=2000, n_classifiers=200.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "%matplotlib notebook\n",
    "\n",
    "def gen_data(n_samples):\n",
    "    \"\"\"\n",
    "    Generate data.\n",
    "    \n",
    "    INPUT:\n",
    "        n_samples : number of samples to generate (observations)\n",
    "        \n",
    "    OUTPUT:\n",
    "       data : 2D data points\n",
    "       labels : ndarray of class labels with integer values -1 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    data = np.random.random((n_samples,2)) - 0.5\n",
    "    labels = np.linalg.norm(data, axis=1) < 0.4\n",
    "    labels = (labels.astype(int) * 2) - 1\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "class Classifier(object):\n",
    "    \n",
    "    def __init__(self, dimension=0, threshold=0., sign=1):\n",
    "        super().__init__()\n",
    "        self.dimension = dimension\n",
    "        self.threshold = threshold\n",
    "        self.sign = sign\n",
    "        \n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Predict new data\n",
    "\n",
    "        INPUT:\n",
    "            data : 2D data points\n",
    "\n",
    "        OUTPUT:\n",
    "            estimated labels\n",
    "        \"\"\"\n",
    "        return self.sign * ((data[:, self.dimension] >= self.threshold).astype(int) * 2 - 1)\n",
    "    \n",
    "def train_classifier(classifier, data, targets, weights):\n",
    "    \"\"\"\n",
    "    Predict new data\n",
    "\n",
    "    INPUT:\n",
    "        classifier : classifier to train\n",
    "        data : 2D data points\n",
    "        targets : ndarray of class labels with integer values -1 or 1\n",
    "        weights: weights assigned to each data sample\n",
    "\n",
    "    OUTPUT:\n",
    "        classifier : trained classifier\n",
    "    \"\"\"\n",
    "    min_error = np.inf\n",
    "    dimensions = [0, 1]\n",
    "    thresholds = np.linspace(-0.5, 0.5, 11)\n",
    "    signs = [-1, 1]\n",
    "    best_parameters = (0, 0., 1)\n",
    "\n",
    "    for d in dimensions:\n",
    "        classifier.dimension = d\n",
    "        for t in thresholds:\n",
    "            classifier.threshold = t\n",
    "            for s in signs:\n",
    "                classifier.sign = s\n",
    "                \n",
    "                predictions = classifier.predict(data)\n",
    "                error = (weights * (predictions != targets).astype(int)).sum()\n",
    "                \n",
    "                if error < min_error:\n",
    "                    best_parameters = (d, t, s)\n",
    "                    min_error = error\n",
    "    \n",
    "    classifier.dimension = best_parameters[0]\n",
    "    classifier.threshold = best_parameters[1]\n",
    "    classifier.sign = best_parameters[2]\n",
    "    return classifier\n",
    "\n",
    "class Animation(object):\n",
    "    \"\"\"\n",
    "    Creates animation object to visualize the decision boundaries of the strong classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels, decision_boundaries):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            data : 2D data points\n",
    "            labels : ndarray of class labels with integer values -1 or 1\n",
    "            decision_boundaries: ndarray of shape (n_classifiers, n**2) with prediction results for an (n x n) grid.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.decision_boundaries = decision_boundaries\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        \n",
    "        # plot data points\n",
    "        ax.scatter(data[labels==-1,1], data[labels==-1,0], c=np.array([[0.5, 0.7, 1.]]), marker='o')\n",
    "        ax.scatter(data[labels==1,1], data[labels==1,0], c=np.array([[1., 0.6, 0.6]]), marker='x')\n",
    "        # write axis labels\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        \n",
    "        x_lim = ax.get_xlim()\n",
    "        y_lim = ax.get_ylim()\n",
    "        im = ax.imshow(decision_boundaries[0], extent=(-0.55,0.55,-0.55,0.55), interpolation='none', cmap='jet', origin='lower', vmin=-1, vmax=1)\n",
    "        ax.set_xlim(x_lim)\n",
    "        ax.set_ylim(y_lim)\n",
    "        \n",
    "        fig.colorbar(im)\n",
    "        \n",
    "        self.fig = fig\n",
    "        self.im = im\n",
    "        \n",
    "    def animate(self, i):\n",
    "        \"\"\"\n",
    "        animate function for FuncAnimation\n",
    "        \n",
    "        Input:\n",
    "            i : integer for indexing\n",
    "        \"\"\"\n",
    "        self.im.set_array(self.decision_boundaries[i])\n",
    "        \n",
    "    def get_animation(self):\n",
    "        \"\"\"\n",
    "        Return animation object which holds the animation\n",
    "        \"\"\"\n",
    "        return FuncAnimation(self.fig, self.animate, frames=len(self.decision_boundaries))\n",
    "\n",
    "def get_decision_boundary(data, classifiers, alphas):\n",
    "    \"\"\"\n",
    "    Create the decision boundary of a strong classifier. The function creates a 12 x 12 grid and a strong classifier. For every location in the grid the prediction is obtained and returned.\n",
    "    \n",
    "    INPUT:\n",
    "        data : 2D data points\n",
    "        classifiers : list of trained classifiers\n",
    "        alphas : ndarray with weights of the classifiers\n",
    "\n",
    "    OUTPUT:\n",
    "        decision_boundary : ndarray of shape (n_classifiers, 12**2) with prediction results for a (12 x 12) grid.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_classifiers = len(classifiers)\n",
    "    \n",
    "    # create grid\n",
    "    nx = 12\n",
    "    ny = 12\n",
    "    x = np.linspace(-0.5, 0.5, nx)\n",
    "    y = np.linspace(-0.5, 0.5, ny)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    xy = np.stack((xx.reshape(-1),yy.reshape(-1)), 1)\n",
    "    \n",
    "    # classify grid points\n",
    "    predictions = np.zeros((ny * nx, n_classifiers))\n",
    "    for i, classifier in enumerate(classifiers):\n",
    "        prediction = classifier.predict(xy)\n",
    "        predictions[:,i] = prediction\n",
    "    decision_boundary = ((alphas[None,:n_classifiers] * predictions).sum(axis=1) > 0).astype(int) * 2 - 1\n",
    "    decision_boundary = decision_boundary.reshape(ny, nx)\n",
    "    \n",
    "    return decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Choose hyperparameters\n",
    "\"\"\"\n",
    "# TODO set number of samples (observations)\n",
    "n_samples = 2000\n",
    "\n",
    "# TODO set number of rounds\n",
    "n_classifiers = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train, combine, and visualize multiple weak classifiers \n",
    "\"\"\"\n",
    "\n",
    "# TODO generate data\n",
    "data, targets = pass\n",
    "\n",
    "# TODO initialize sample weights\n",
    "weights = pass\n",
    "\n",
    "# TODO initialize arrays or lists for storing the alphas, predictions, decision_boundaries, errors and classifiers\n",
    "\n",
    "for t in range(n_classifiers):\n",
    "    # TODO train weak classifier\n",
    "    classifier = Classifier()\n",
    "    \n",
    "    # TODO compute predictions of the weak classifier\n",
    "    \n",
    "    # TODO calculate the weighted! training error of the weak classifier\n",
    "    \n",
    "    # TODO calculate alpha\n",
    "    \n",
    "    # TODO update weight distribution for samples (note: normalization)\n",
    "    \n",
    "    # TODO calculate the error of the final strong classifier (so far)\n",
    "    \n",
    "    # TODO compute the decision boundary of the current strong classifier (so far) \n",
    "    \n",
    "# TODO plot the error of the final strong classifiers for each round\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = Animation(data, targets, decision_boundaries)\n",
    "HTML(animation.get_animation().to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2: Filter Matrix\n",
    "\n",
    "Given a $2 \\times 4$ image\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "    a &b &c &d \\\\\n",
    "    e &f &g &h \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "and a $1 \\times 3$ filter kernel\n",
    "\\begin{equation}\n",
    "\\mathbf{K} = \n",
    "\\begin{bmatrix}\n",
    "    -1 &0 &1 \n",
    "\\end{bmatrix},\n",
    "\\end{equation}\n",
    "write down the [filter operation](https://www.deeplearningbook.org/contents/convnets.html) as a matrix multiplication $\\mathbf{y} = \\mathbf{W}_{k} \\mathbf{x}$.\n",
    "\n",
    "1. What is the output image $\\mathbf{Y}$ when filtering $\\mathbf{X}$ with $\\mathbf{K}$?\n",
    "2. Flatten $\\mathbf{X}$ into an 8-dimensional vector ($8 \\times 1$) $\\mathbf{x}$ by concatenating each matrix row.\n",
    "3. Create a matrix $\\mathbf{W}_{k}$ such that $\\mathbf{W}_{k} \\mathbf{x}$ returns a 4-dimensional vector $\\mathbf{y}$ that can be reshaped into $\\mathbf{Y}$.\n",
    "4. For a filter operation with the same dimensions as in (3.), how does the weight matrix $\\mathbf{W}$ for any single $1 \\times 3$ kernel of a convolution layer look like?\n",
    "5. Can a hidden layer of a multilayer perceptron (MLP) learn the filter operation mentioned above? What are the differences between the weight matrix of a hidden layer $\\mathbf{H}$ and a convolution layer $\\mathbf{W}$?\n",
    "6. Suppose you have two classification models for a two-class dataset consisting of several $2 \\times 4$ images: an MLP $\\sigma ( \\mathbf{M}(ReLU(\\mathbf{H} \\mathbf{x})))$ and a convolutional neural network (CNN) $\\sigma ( \\mathbf{M}(ReLU(\\mathbf{W} \\mathbf{x})))$, both having an additional $4 \\times 1$ output matrix $\\mathbf{M}$. With $\\sigma$ being the sigmoid function, both models produce a score $s \\in [0, 1]$ denoting whether an input image $\\mathbf{x}$ belongs to a class A ($s=1$) or to a class B ($s=0$). After training, both models reach a training accuracy of 100%. Without further testing, which model would you prefer to use in production and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following comprehension questions either with right or wrong and briefly explain your decision:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Multiplying the matrix output with a negative scalar is a valid non-linearity for deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. If the input data are distributed uniformly, and a small latent dimension is chosen, an Autoencoder behaves similarly to a PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. To obtain a faster, albeit slightly less effective, version of AdaBoost, one can omit readjusting the sample weights during an iteration step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To effectively train a CNN committee, different training datasets (bagging) or architectures are needed; otherwise, each network learns the same weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. To separate five different classes with AdaBoost, you need to train 5 AdaBoost classifiers for one-versus-all and 9 AdaBoost classifiers for one-versus-one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
