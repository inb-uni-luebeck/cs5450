{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "The following code downloads and imports all necessary files and modules into the virtual machine of Colab. Please make sure to execute it before solving this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "  if os.getcwd() == '/content':\n",
    "    !git clone 'https://github.com/inb-uni-luebeck/cs5450.git'\n",
    "    os.chdir('cs5450')\n",
    "\n",
    "#making sure livelossplot is installed\n",
    "try:\n",
    "    import livelossplot\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install livelossplot==0.4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This exercise can utilize GPU acceleration. If you are using Google Colab you can enable access to a cloud GPU by selecting from the menu above: \n",
    "\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU**\n",
    "\n",
    "If you are running this notebook on your own machine, GPU acceleration is available if you have an Nvidia GPU and a CUDA-enabled driver installed. Otherwise calculations will run on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iceberg vs. Ship\n",
    "In this exercise, we will use deep learning to solve a real-world problem: classifying whether a given grayscale representation of a radar image is a ship or an iceberg. To make things more interesting, this exercise sheet will be carried out as a competition.\n",
    "\n",
    "Besides the labeled training set of $1000$ images, an unlabeled test set of $604$ images is provided. The dimension of each grayscale image is $30 \\times 30$ pixels. An example image of both a ship and an iceberg is given in the figure below.\n",
    "  \n",
    "The next cells provide code to get you started. It contains a fully functional implementation of a simple convolutional neural network (CNN). It has a baseline accuracy of $79.50\\%$ on the fixed validation set. The architecture of the network is given in table below. Your task is to extend the code, thus optimizing the classification capabilities of the network.\n",
    "\n",
    "To evaluate the performance of your network, the average **log loss** is used. For a single sample $\\vec{x}$ with true binary label $y \\in \\left\\{ 0,1 \\right\\}$ and an estimated posterior probability $p \\in \\left[ 0,1 \\right]$, the log loss is defined as:\n",
    "  \n",
    "  $ E \\left( \\vec{x} \\right) = - \\left[ y \\log p + \\left( 1 - y \\right) \\log \\left( 1 - p \\right) \\right].$\n",
    "\n",
    "Your final submission should be a $604$-dimensional vector: the probability that the given sample is an iceberg (label $1$) for each sample of the test set. \n",
    "\n",
    "Iceberg | Ship    \n",
    "- | - \n",
    "<img src=\"data/iceberg.jpg\" width=\"300\"/> | <img src=\"data/ship.jpg\" width=\"300\"/>\n",
    "\n",
    "| Layer       | Kernel | Stride |    Shape   | #Params |\n",
    "|-------------|:------:|:------:|:----------:|:-------:|\n",
    "| Input       |    -   |    -   |      -     |    0    |\n",
    "| Convolution |  (3,3) |  (1,1) |  (30,30,1) |   160   |\n",
    "| Pooling     |  (2,2) |  (2,2) | (28,28,16) |    0    |\n",
    "| Linear      |    -   |    -   |     (2)    |   6274  |\n",
    "\n",
    "\n",
    "## Hints\n",
    "In case you are struggeling with the task, here are some helpful tips and hints:\n",
    "1. Useful pytorch [overview](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py)\n",
    "2. [Augment](https://pytorch.org/docs/stable/torchvision/transforms.html) the training data (e.g., mirroring).\n",
    "3. Make the network deeper (e.g., more convolution and pooling layers)\n",
    "4. Regularize your network (e.g., with dropout layers).\n",
    "5. Use cross-validation instead of a fixed train-validation split.\n",
    "6. Train multiple networks and combine them in an ensemble.\n",
    "7. Post-process your posterior probabilities (e.g., is log 0 a good idea?)\n",
    "\n",
    "## Further notes and remarks:\n",
    "1. Each team has to choose a team name. This will be the anonymous identifier when\n",
    "the competition results are published."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from livelossplot import PlotLosses\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    X = np.load(path)[...,0,:].astype('float32')\n",
    "    X -= X.min()\n",
    "    X = (255.*X/X.max()).astype('uint8')\n",
    "    X = np.swapaxes(X,0,2)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_aug = test_augmentation = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "\n",
    "class IcebergDataset(Dataset):\n",
    "    def __init__(self, X, Y=None, augmentation=default_aug, is_test=False):\n",
    "        if Y is None:\n",
    "            assert is_test\n",
    "            \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.augmentation = augmentation\n",
    "        \n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.X[index, ...]\n",
    "        \n",
    "        x = self.augmentation(x)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return x\n",
    "        y = self.Y[index]\n",
    "        y = torch.tensor(y).float()\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ------------Architecture------------------------\n",
    "#-------------------------------------------------\n",
    "\n",
    "def make_conv_layer(in_dim, out_dim, kernel_size=3, **kwargs):\n",
    "    # this \"conv_layer\" is actually a composition of convolution-layer, relu and batch-normalization\n",
    "    conv_layer = nn.Sequential()\n",
    "    \n",
    "    # NOTE: kwargs could be a dictionary like {'padding':1}\n",
    "    # you can call the function like e.g.:\n",
    "    # make_conv_layer(in_dim, out_dim, kernel_size=3, **{'padding':1}) \n",
    "    conv_layer.add_module('conv', nn.Conv2d(in_dim, out_dim, kernel_size, **kwargs))\n",
    "    conv_layer.add_module('relu', nn.ReLU())\n",
    "    conv_layer.add_module('batch_norm', nn.BatchNorm2d(out_dim))\n",
    "    \n",
    "    return conv_layer\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # in: (b, 1, 30, 30) b= batch_size\n",
    "        self.conv_0 = make_conv_layer(1, 16, 3)\n",
    "        # in: (b, 16, 28, 28)\n",
    "        self.pool_0 = nn.MaxPool2d(2, 2) # kernel -> (2, 2), stride -> (2, 2)\n",
    "        \n",
    "        # in: (b, 16, 14, 14) -> will be flattened to (b, 16*14*14) = (b, 3136)\n",
    "        self.linear = nn.Linear(16*14*14, 1)\n",
    "        # ouput: (b, 1) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # forward is the function that is called when using prediction = model(x)\n",
    "        \n",
    "        x = self.conv_0(x)\n",
    "        x = self.pool_0(x)\n",
    "\n",
    "        # flatten before fully connected layer \n",
    "        x = x.flatten(start_dim=1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x[:,0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(opt_id, parameters, learning_rate, **kwargs):\n",
    "    if opt_id == 'SGD':\n",
    "        optimizer = optim.SGD(parameters,\n",
    "                              lr=learning_rate,\n",
    "                              momentum=kwargs.get('momentum', .9),\n",
    "                              weight_decay=kwargs.get('weight_decay', 0.)\n",
    "                              )\n",
    "    elif opt_id == 'Adam':\n",
    "        optimizer = optim.Adam(\n",
    "            parameters,\n",
    "            lr=learning_rate,\n",
    "            weight_decay=kwargs.get('weight_decay', 0.)\n",
    "        )\n",
    "    \n",
    "    return optimizer\n",
    "        \n",
    "def get_scheduler(lr_steps, epochs, optimizer, gamma=.1):\n",
    "    assert lr_steps < epochs, 'Epochs must be greater than lr_steps'\n",
    "    step_size = epochs // lr_steps\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size,\n",
    "        gamma=gamma, last_epoch=-1\n",
    "    )\n",
    "\n",
    "    return scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ------------Hyperparameters---------------------\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Data set params: run the Train & Test Data cell for changes here to take place!\n",
    "batch_size = 32\n",
    "perc_test_set = .2\n",
    "\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 0.\n",
    "num_sched_steps = 2\n",
    "epochs = 120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ------------Train & Test Data-------------------\n",
    "#-------------------------------------------------\n",
    "\n",
    "# images need to be real uint8 images in [0, 255]\n",
    "\n",
    "X = load_data('data/train_data.npy')\n",
    "Y = np.load('data/train_labels.npy')[:,0]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=perc_test_set, random_state=42)\n",
    "\n",
    "mu = (X_train/255).mean()\n",
    "sigma = (X_train/255).std()\n",
    "normalize = transforms.Normalize(mean=[mu,], std=[sigma,])\n",
    "\n",
    "train_augmentation = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "            \n",
    "        ])\n",
    "\n",
    "\n",
    "test_augmentation = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "\n",
    "data_loader_train = DataLoader(\n",
    "    IcebergDataset(X_train,Y_train, train_augmentation),\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "data_loader_val = DataLoader(\n",
    "    IcebergDataset(X_test,Y_test, test_augmentation),\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": data_loader_train,\n",
    "    \"validation\": data_loader_val\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# early stopping\n",
    "# submission code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on cuda if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "liveloss = PlotLosses(plot_extrema=False)\n",
    " \n",
    "#-------------------------------------------------\n",
    "# ------------Setup-----------------------------\n",
    "#-------------------------------------------------\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "optimizer = get_optimizer(\n",
    "    'SGD', model.parameters(), learning_rate,\n",
    "    **{'weight_decay':weight_decay, 'momentum':0.9}\n",
    ")\n",
    "\n",
    "\n",
    "#scheduler = get_scheduler(num_sched_steps, epochs, optimizer)\n",
    "scheduler = None\n",
    "\n",
    "#-------------------------------------------------\n",
    "# ------------Training-----------------------------\n",
    "#-------------------------------------------------\n",
    "\n",
    "loss_fcn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    logs = {}\n",
    "\n",
    "    # Run a training epoch, then a validation epoch\n",
    "    for phase in ['train', 'validation']:\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.\n",
    "        counter = 0.\n",
    "    \n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "        \n",
    "        for x, label in dataloaders[phase]:\n",
    "\n",
    "            if phase == 'train':\n",
    "                prediction = model(x.to(device))\n",
    "                loss = loss_fcn(prediction, label.to(device))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    prediction = model(x.to(device))\n",
    "                    loss = loss_fcn(prediction, label.to(device))\n",
    "                    \n",
    "\n",
    "            preds = (torch.sigmoid(prediction) >= .5).float().cpu()\n",
    "            counter += float(x.shape[0])\n",
    "            running_loss += loss.item()\n",
    "            running_corrects += torch.sum(preds == label).item()\n",
    "\n",
    "            \n",
    "        prefix = ''\n",
    "        if phase == 'validation':\n",
    "            prefix = 'val_'\n",
    "        \n",
    "        logs[prefix + 'loss'] = running_loss / counter\n",
    "        logs[prefix + 'acc'] = running_corrects / counter\n",
    "\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "    \n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "# ------------Test Submission---------------------\n",
    "#-------------------------------------------------\n",
    "\n",
    "team_name = 'baseline'\n",
    "if team_name == 'baseline':\n",
    "    print(\"Don't forget to change the team name for your submission.\")\n",
    "    \n",
    "test_data = load_data('data/test_data.npy')\n",
    "test_augmentation = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "dataset_test = IcebergDataset(\n",
    "    test_data,\n",
    "    augmentation=test_augmentation,\n",
    "    is_test=True\n",
    ")\n",
    "\n",
    "data_loader_test = DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions_ = []\n",
    "# save the model output directly to numpy file, output shape should be (604,)\n",
    "# How to find the submission in Colab?\n",
    "# Click on the folder icon on the left side of the screen.\n",
    "# Your file should be located in:\n",
    "# /content/cs5450/data/[some_name]_submission.npy\n",
    "# Right-click on the file and left-click on \"Download\"\n",
    "for x in data_loader_test:\n",
    "    prediction = model(x.to(device))\n",
    "    predictions_.append(prediction.detach().cpu().numpy())\n",
    "    \n",
    "predictions_ = np.concatenate(predictions_,0)\n",
    "np.save('data/' + team_name + '_submission.npy',predictions_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your submission file should be named [team_name]_submission.npy!\n",
    "## Provide a file `[team_name]_comment.txt` that briefly explains your approach!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
