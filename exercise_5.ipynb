{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please insert the names of all participating students:\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "  if os.getcwd() == '/content':\n",
    "    !git clone 'https://github.com/inb-luebeck/cs5450.git'\n",
    "    os.chdir('cs5450')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use the random forest algorithm to combine multiple decision trees to one strong classifier for a multi-class classification problem on a two-dimensional dataset. The individual trees of the random forest differ because they are trained with a random subset of (i) features — in this exercise a single feature of the two-dimensional dataset\n",
    "— and (ii) training data. The latter is done by applying the technique of *bootstrap aggregating (bagging)*. Given a dataset $D$ of size n, bagging generates m new training sets $D_i$ , each of size n, by sampling from $D$ uniformly and with replacement. Note\n",
    "that by sampling with replacement, some observations may be repeated in $D_i$ and some observations are not used at all. They are called *out-of-bag (oob) observations* and can\n",
    "be used for testing.\n",
    "\n",
    "We will build on the code which we have provided here. You only need to write code at the places indicated by TODO in the files.\n",
    "Implement the missing parts in this file. When done correctly, your code should do the following:\n",
    "\n",
    "1. Create a random dataset.\n",
    "\n",
    "2. Iterativly, do the following:\n",
    "    - Randomly select a single feature.\n",
    "    - Randomly generate a bootstrapped training set.\n",
    "    - Train a decision tree on the bootstrapped set with a single feature\n",
    "    - Generate a test set using the out-of-bag samples.\n",
    "    - Compute the posterior probabilities of the test set.\n",
    "    - Combine the out-of-bag posterior probabilities over the forest to obtain a prediction for each sample of the complete dataset.\n",
    "    - Calculate the out-of-bag error of the random forest.\n",
    "    - Visualize the data and the decision boundaries of the random forest.\n",
    "\n",
    "3. Plot the out-of-bag error of the random forest for each iteration.\n",
    "\n",
    "4. Good initial values are: `n_samples=500`, `n_classes=5`, `n_trees=100`, `n_splits=4`, `seed=20`.\n",
    "\n",
    "You can visualize a decision tree with the function `some_tree.plot_tree`\n",
    "\n",
    "Helpful Documentation:\n",
    "- [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [python sets](https://docs.python.org/2/library/sets.html)\n",
    "- [random integers](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.randint.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn import tree as scikit_tree\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import FuncAnimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#################### DATA #############################\n",
    "#######################################################\n",
    "\n",
    "def gen_data(n_samples, n_classes):\n",
    "    \"\"\"\n",
    "     GENDATA Generate data and its labels.\n",
    "    \n",
    "     INPUT:\n",
    "       n_samples : number of samples (observations)\n",
    "       n_classes : number of distinct classes\n",
    "    \n",
    "     OUTPUT:\n",
    "       data : two-dimensional data points\n",
    "       labels : multi-class labels (i.e. 1, 2, 3, ...) of the data points\n",
    "    \"\"\"\n",
    "    \n",
    "    center = np.random.rand(n_classes, 2)\n",
    "    data = np.random.rand(n_samples, 2)\n",
    "    labels = np.ones(n_samples)\n",
    "    n_samples_per_class = int(n_samples / n_classes)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        r = np.random.randn(n_samples_per_class, 2) / 12.\n",
    "        b = i * n_samples_per_class\n",
    "        e = (i+1)*n_samples_per_class\n",
    "        data[b:e, :] = center[i, :] + r\n",
    "        labels[b:e] = i\n",
    "\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "################ VISUALIZATION ########################\n",
    "#######################################################\n",
    "\n",
    "def get_decision_boundary(data, labels, trees_):\n",
    "    xrange = [data[:,0].min(), data[:,0].max()]\n",
    "    yrange = [data[:,1].min(), data[:,1].max()]\n",
    "    inc = 0.01;\n",
    "    x, y = np.meshgrid(np.arange(xrange[0], xrange[1],inc), np.arange(yrange[0], yrange[1],inc));\n",
    "    image_shape = x.shape;\n",
    "    xy = np.stack([x.flatten(), y.flatten()],-1);\n",
    "\n",
    "    n_samples = x.size #numel(x);\n",
    "    n_classes = (np.unique(labels)).shape[0];\n",
    "\n",
    "    predictions = np.zeros((n_samples, n_classes))\n",
    "    for tree in trees_:\n",
    "        random_feature = tree.feature\n",
    "        s = tree.predict_proba(xy[:, random_feature])\n",
    "        predictions = predictions + s;\n",
    "\n",
    "    predictions = predictions / float(len(trees_));\n",
    "    c = predictions.argmax(1)\n",
    "    a = predictions.max(1).reshape(image_shape)\n",
    "\n",
    "    c = c.reshape(image_shape)\n",
    "    return c\n",
    "\n",
    "class Animation(object):\n",
    "    \"\"\"\n",
    "    Creates animation object to visualize the decision boundaries of the strong classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, labels, decision_boundaries):\n",
    "        \"\"\"\n",
    "        INPUT:\n",
    "            data : 2D data points\n",
    "            labels : ndarray of class labels with integer values -1 or 1\n",
    "            decision_boundaries: ndarray of shape (n_classifiers, n**2) with prediction results for an (n x n) grid.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.decision_boundaries = decision_boundaries\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "        \n",
    "        xrange = [data[:,0].min(), data[:,0].max()]\n",
    "        yrange = [data[:,1].min(), data[:,1].max()]\n",
    "        cmap = plt.cm.get_cmap('jet',lut=n_classes)\n",
    "\n",
    "        for i in range(n_classes):\n",
    "            tmp = data[labels==i,:]\n",
    "            plt.scatter(*tmp.T, marker='d',color=cmap(i), s=150, edgecolor='k')\n",
    "        \n",
    "        im = ax.imshow(decision_boundaries[0], extent=xrange + yrange,\n",
    "                       interpolation='none', cmap='jet', origin='lower',\n",
    "                       vmin=0, vmax=n_classes-1,\n",
    "                       alpha=.7)\n",
    "        \n",
    "        self.fig = fig\n",
    "        self.im = im\n",
    "        \n",
    "    def animate(self, i):\n",
    "        \"\"\"\n",
    "        animate function for FuncAnimation\n",
    "        \n",
    "        Input:\n",
    "            i : integer for indexing\n",
    "        \"\"\"\n",
    "        self.im.set_array(self.decision_boundaries[i])\n",
    "        \n",
    "    def get_animation(self):\n",
    "        \"\"\"\n",
    "        Return animation object which holds the animation\n",
    "        \"\"\"\n",
    "        return FuncAnimation(self.fig, self.animate, frames=len(self.decision_boundaries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "################ CLASSIFIER ###########################\n",
    "#######################################################\n",
    "\n",
    "class MyTree():\n",
    "    def __init__(self, tree, feature):\n",
    "        self.clf = tree\n",
    "        self.feature = feature\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        assert x.ndim == 1\n",
    "        x = x.reshape(x.shape[0], 1)\n",
    "        return self.clf.predict_proba(x)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "        \n",
    "    def plot_tree(self):\n",
    "        scikit_tree.plot_tree(self.clf)\n",
    "        \n",
    "def train_tree(data, labels, feature, n_splits):\n",
    "    \"\"\"\n",
    "     TRAINTREE Train a single decision tree.\n",
    "    \n",
    "     INPUT:\n",
    "       data : one-dimensional data points\n",
    "       labels : multi-class labels of the data points\n",
    "       feature : the randomly selected feature (i.e. 1 or 2)\n",
    "       n_splits : maximum depth\n",
    "    \n",
    "     OUTPUT:\n",
    "       tree : trained decision tree\n",
    "    \"\"\"\n",
    "    assert data.ndim == 1,'After random feature selection, trainData must be single-column'\n",
    "    data = data.reshape((data.shape[0], 1))\n",
    "    \n",
    "    tree = scikit_tree.DecisionTreeClassifier(max_depth=n_splits, criterion='entropy')\n",
    "    tree = tree.fit(data, labels)\n",
    "    \n",
    "    return MyTree(tree, feature)\n",
    "#tree = fitctree(data, labels, 'MaxNumSplits', nSplits, 'SplitCriterion', 'deviance', 'PredictorNames', int2str(feature));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define variables\n",
    "\n",
    "# TODO nSamples : number of samples (observations)\n",
    "n_samples = 500;\n",
    "\n",
    "# TODO nClasses : number of classes\n",
    "n_classes = 5;\n",
    "\n",
    "# TODO nTrees : number of trees\n",
    "n_trees = 100;\n",
    "\n",
    "# TODO nSplits : maximum number of decision splits of the tree\n",
    "n_splits = 3;\n",
    "\n",
    "# TODO seed : set seed for reproducable results\n",
    "seed = 20;\n",
    "\n",
    "#rng(seed);\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train, combine, and visualize multiple decision trees\n",
    "\n",
    "# TODO generate data (help genData)\n",
    "data, labels = -1, -1\n",
    "\n",
    "trees_ = list()\n",
    "posterior = np.zeros((n_samples, n_trees, n_classes))\n",
    "errors = list()\n",
    "decision_boundaries = []\n",
    "\n",
    "# loop number of trees\n",
    "for t in range(n_trees):\n",
    "    \n",
    "    # TODO select a random feature (i.e. 1 or 2) of the two-dimensional dataset\n",
    "    feature = -1\n",
    "    \n",
    "    # TODO randomly sample 'n_samples' data points and its labels with replacement\n",
    "    # This is called bootstrap aggregating (bagging)\n",
    "    train_idx = np.random.randint(0,high=n_samples, size=n_samples)\n",
    "    train_data = data[train_idx, feature]\n",
    "    train_labels = labels[train_idx]\n",
    "    \n",
    "    # TODO train the decision tree (train_tree)\n",
    "    trees_.append(\n",
    "        -1\n",
    "    )\n",
    "    \n",
    "    # TODO use the remaining samples to test the tree (set difference)\n",
    "    # These are called out-of-bag (oob) observations\n",
    "    test_idx = -1\n",
    "    test_data = -1\n",
    "    test_labels = -1\n",
    "    \n",
    "    # TODO compute the oob posterior probabilities\n",
    "    # (see MyTree & scikit DecisionTreeClassifier)\n",
    "    scores = -1\n",
    "    posterior[0,0,0] = 0\n",
    "    \n",
    "    # TODO compute the class predictions of the random forest (so far)\n",
    "    predictions = -1\n",
    "    \n",
    "    # TODO compute the oob error of the random forest (so far)\n",
    "    errors.append(\n",
    "        -1\n",
    "    )\n",
    "    \n",
    "    # TODO visualize the data and the current decision boundaries (help showData)\n",
    "    #show_data(data, labels, trees_);\n",
    "    decision_boundaries.append(-1)\n",
    "\n",
    "\n",
    "# TODO plot the oob error of the random forest for each iteration\n",
    "plt.figure()\n",
    "plt.plot(errors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animation = Animation(data, labels, decision_boundaries)\n",
    "HTML(animation.get_animation().to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot one of the trees\n",
    "plt.figure(figsize=(20,20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Unlike a SVM, a decision tree is intrinsically suited for a multi-class problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A decision tree cannot be used for regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pure nodes of a decision tree (i.e. only a single class is present) have a high entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. A decision stump (a one-level decision tree) is prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Pruning a decision tree can be useful to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Each decision tree in a random forest is identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. The expected fraction of unique observations in a bootstrap dataset is approximately $63.2 \\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. The out-of-bag error estimates the generalization error of the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. A random forest can be both trained and tested in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PT3",
   "language": "python",
   "name": "pt3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
