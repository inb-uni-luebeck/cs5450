{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Please insert the names of all participating students:\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "  if os.getcwd() == '/content':\n",
    "    !git clone 'https://github.com/inb-luebeck/cs5450.git'\n",
    "    os.chdir('cs5450')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.1: Multi-dimensional Scaling (MDS)\n",
    "\n",
    "In this exercise, we will implement Multi-dimensional Scaling. \n",
    "\n",
    "Implement the missing parts below. When done correctly, your code should do the following:\n",
    "1. Generate a swiss roll dataset.\n",
    "2. Compute the squared euclidean distances between samples.\n",
    "3. Compute the gram matrix from squared distances and the centering matrix.\n",
    "4. Sort the eigenvalues and corresponding eigenvectors of the gram matrix.\n",
    "5. Calculate the MDS data representation with reduced dimensions.\n",
    "6. Show the lower-dimensional representation of the data.\n",
    "\n",
    "In case you are struggeling with the task, here are some helpful tips and hints:\n",
    "1. Useful functions: [`make_swiss_roll`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_swiss_roll.html), [`pdist`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html), [`squareform`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.squareform.html), [`numpy.eye`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html), [`numpy.ones`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ones.html), [`numpy.linalg.eig`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.eig.html), [`numpy.argsort`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html), [`numpy.sqrt`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sqrt.html), [`numpy.diag`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.diag.html), [`Axes.scatter`](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.axes.Axes.scatter.html#matplotlib.axes.Axes.scatter), [`Axes3D.scatter`](https://matplotlib.org/3.1.1/api/_as_gen/mpl_toolkits.mplot3d.axes3d.Axes3D.html#mpl_toolkits.mplot3d.axes3d.Axes3D.scatter)\n",
    "\n",
    "2. Additional references: [Color maps for matplotlib](https://matplotlib.org/examples/color/colormaps_reference.html)\n",
    "\n",
    "Note: Don't use the MDS implementation provided by the sklearn package here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "#%matplotlib inline\n",
    "\n",
    "# ---------- USE FOR INTERACTIVE PLOTS --------------------------\n",
    "#%matplotlib notebook \n",
    "%matplotlib nbagg\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "## Define variables\n",
    "\n",
    "# TODO n_samples : number of random samples (observations)\n",
    "n_samples = \n",
    "\n",
    "# n_dims : number of dimensions of the embedding\n",
    "n_dims = 2\n",
    "\n",
    "## Compute and visualize the embedding using MDS\n",
    "\n",
    "# TODO generate data (make_swiss_roll)\n",
    "data, color = \n",
    "\n",
    "# TODO compute euclidean distance vector (pdist)\n",
    "distances = \n",
    "\n",
    "# TODO reshape into distance matrix (squareform)\n",
    "distances = \n",
    "\n",
    "# TODO square the distances inside the matrix\n",
    "distances_squared = \n",
    "\n",
    "# TODO calculate the centering matrix H (c.f. page 15)\n",
    "I = \n",
    "O = \n",
    "centering = \n",
    "\n",
    "# TODO compute the gram matrix B (c.f. page 15)\n",
    "gram = \n",
    "\n",
    "# TODO compute its eigenvectors and eigenvalues (eig)\n",
    "eig_values, eig_vectors = \n",
    "\n",
    "# TODO sort the eigenvalues and its corresponding eigenvectors (descending!)\n",
    "permutation =  # calculate permutation indices for the sorted eigenvalues\n",
    "eig_vectors =  # rearrange eigenvectors according to permutation\n",
    "eig_values =  # rearrange eigenvalues according to permutation\n",
    "\n",
    "# TODO calculate the square root of the eigenvalues (note: add offset to avoid negative numbers from small numerical errors)\n",
    "epsilon = 0.001\n",
    "eig_values_root = \n",
    "\n",
    "# TODO reduce dimensions (c.f. equation 1.20)\n",
    "data_mds = \n",
    "\n",
    "# TODO visualize both the original data and its embedding (add_subplot, scatter)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize = (12,6))\n",
    "ax = fig.add_subplot(121, projection='3d') #get Axes3D object for subplot\n",
    "ax. #3d plot here\n",
    "\n",
    "ax = fig.add_subplot(122) #get Axes object for subplot\n",
    "ax. # 2d plot here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Isomap\n",
    "\n",
    "In this exercise, we will be implementing the Isomap algorithm.\n",
    "\n",
    "You can use the MDS implementation from sklearn within your Isomap implementation.\n",
    "\n",
    "Implement the missing parts below. When done correctly, your code should do the following:\n",
    "1. Generate a swiss roll dataset.\n",
    "2. Compute the euclidean distances between samples.\n",
    "3. Sort the neighbors of each sample by distance.\n",
    "4. Keep the closest k neighbors of each sample and set all other distances to infinity or NaN or zero to mark them as not-connected.\n",
    "5. Calculate all shortest paths between pairs of samples (representing geodesic distances on the manifold).\n",
    "6. Calculate the MDS data representation with reduced dimensions on the geodesic distances.\n",
    "7. Show the lower-dimensional representation of the data.\n",
    "\n",
    "In case you are struggeling with the task, here are some helpful tips and hints:\n",
    "1. Useful functions:[`csgraph_from_dense`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.csgraph_from_dense.html), [`shortest_path`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csgraph.shortest_path.html#scipy.sparse.csgraph.shortest_path), [`MDS`](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html)\n",
    "2. Make sure to use MDS with `dissimilarity='precomputed'` so that it takes a distance matrix as input instead of a matrix of sample coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.sparse.csgraph import csgraph_from_dense, shortest_path\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "## Define variables\n",
    "\n",
    "# TODO n_samples : number of random samples (observations)\n",
    "n_samples = \n",
    "\n",
    "# n_dims : number of dimensions of the embedding\n",
    "n_dims = 2\n",
    "\n",
    "# TODO n_neighbors : number of nearest neighbors (k)\n",
    "n_neighbors = \n",
    "\n",
    "\n",
    "## Compute and visualize the embedding using Isomap\n",
    "\n",
    "# TODO generate data (make_swiss_roll)\n",
    "data, color = \n",
    "\n",
    "# TODO compute euclidean distance vector (pdist)\n",
    "distances = \n",
    "\n",
    "# TODO reshape into distance matrix (squareform)\n",
    "distances = \n",
    "\n",
    "# TODO find the nearest neighbor-indices (argsort)\n",
    "permutation =  # sort along one dim and get indices for permutation\n",
    "\n",
    "\n",
    "# TODO create the local distance matrix (only to the nearest neighbors)\n",
    "# All other values must be zero\n",
    "distances_local = # initialize\n",
    "for i in range(n_samples):\n",
    "   # TODO: write the K-NNs (of the i-th row) distances to the position of K-NNs\n",
    "   indices =\n",
    "   distances_local[i,indices] = \n",
    "\n",
    "# enforce distances_local to be symmetric\n",
    "distances_local_transposed = distances_local.T\n",
    "# get all entries that should be !=0 but are not in the current dist mat\n",
    "mask = (distances_local == 0) & (distances_local_transposed != 0);\n",
    "distances_local[mask] = distances_local_transposed[mask]\n",
    "\n",
    "# TODO compute shortest paths\n",
    "# output: [D(i,j)], where D(i,j) = distance from node 'i' to node 'j' \n",
    "distances_geodesic = \n",
    "assert not np.any(np.isinf(distances_geodesic)), 'Error: manifold is fragmented into multiple disconnected regions.'\n",
    "\n",
    "# TODO classical multidimensional scaling (MDS)\n",
    "embedding = \n",
    "data_isomap = \n",
    "\n",
    "# TODO visualize both the original data and its embedding (add_subplot, scatter)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize = (12,6))\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax. # 3d plot here\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax. # 2d plot here\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.3: Comprehension Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following comprehension questions either with right or wrong and briefly explain your decision:\n",
    "\n",
    "1. A manifold is a topological space that locally resembles Euclidean space near each point, e.g. a sphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The features found by classical MDS are equivalent to the ones computed by ICA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Isomap replaces the Euclidean distance measure used by MDS with geodesic distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Local Linear Embedding (LLE) seeks a lower-dimensional projection of the data trying to preserve the distances within a local neighborhood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Unlike MDS, LLE uses global properties of the manifold to find an embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. $n+1$ non-collinear points of two distinct classes are always linearly separable in $\\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. The reason that the kernel trick can be applied is that the observations only show up in pairs of inner products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. To apply the kernel trick, the function $\\boldsymbol{\\Phi} \\left( \\vec{x} \\right)$ must be explicitly known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. The function $\\boldsymbol{\\Phi} \\left( \\vec{x} \\right) = \\boldsymbol{\\Phi} \\left( \\left[x_1, x_2 \\right] \\right) = \\left[ x_1^2, \\sqrt{2} x_1 x_2, x_2^2\\right]$ is a function of the kernel $K \\left( \\vec{x}, \\vec{y} \\right) = \\langle \\vec{x}, \\vec{y}\\rangle^2$. This function is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Your answer]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
